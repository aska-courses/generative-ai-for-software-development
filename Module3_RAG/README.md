# InsureLLM – AI Knowledge Worker with RAG

## 1. Main Idea

InsureLLM is an internal **AI knowledge worker** for an insurance-focused startup.  
The goal is to let employees ask natural-language questions like:

- “When was InsureLLM founded and by whom?”
- “What are the renewal terms for contracts?”

The AI does **not** rely only on pre-trained knowledge. Instead, it uses **Retrieval-Augmented Generation (RAG)** to read from InsureLLM’s internal knowledge base (KB) stored on a shared drive, and then answers questions grounded in those documents.

All data in this prototype is **synthetic**: short, realistic markdown documents generated by LLMs (GPT, Claude) for safe experimentation.

**VIDEO LINK**: https://youtu.be/BhQMLbyZY18

---
## 2. Key Concepts

- **RAG (Retrieval-Augmented Generation)**  
  Combine a vector search over internal documents with an LLM that generates answers using retrieved context.

- **Knowledge Base (KB)**  
  A set of markdown files organized in four folders: `company/`, `contracts/`, `employees/`, `products/`.

- **Vector Embeddings**  
  Each text chunk is converted into a numeric vector using OpenAI’s `text-embedding-3-small` model.

- **Vector Store**  
  FAISS stores embeddings and enables fast similarity search to find relevant chunks.

- **LLM Orchestrator**  
  A chat model (e.g. `gpt-4o-mini`) receives the user question plus retrieved chunks and generates a grounded answer.

- **User Interface**  
  A Gradio-based web chat where users ask questions and see answers plus (optionally) source snippets.

---

## 3. Dataset Concept

### 3.1 Directory Structure

```text
knowledge-base/
  company/
    about_company.md
    mission_vision.md
    org_structure.md
  contracts/
    master_service_agreement.md
    sme_contract_template.md
    enterprise_renewal_policies.md
  employees/
    ceo_avery_lancaster.md
    employee_jordan_smith_analytics.md
    employee_casey_lee_sales.md
  products/
    insurelm_core_platform.md
    insurelm_claims_assistant.md
    insurelm_risk_scoring_api.md
```

* All documents are **markdown (`.md`)**.
* All content is **fake / synthetic** but **internally consistent**, enabling realistic Q&A.
* Documents are relatively short but **representative** of a real SaaS / insurtech company.
---

## 4. System Design Overview

### 4.1 High-Level Architecture

1. **Ingestion & Indexing**

   * Load all `.md` files from `knowledge-base/`.
   * Add metadata (e.g. `doc_type` from folder path).
   * Split each document into overlapping chunks.
   * Generate embeddings for each chunk using `text-embedding-3-small`.
   * Store embeddings + metadata in a FAISS index.
   * Persist the FAISS index to disk (e.g. `vector_db/`).

2. **Serving & Retrieval**

   * On app startup, load the FAISS index and embeddings client.
   * Initialize a retriever object that can fetch top-k relevant chunks.

3. **LLM & RAG Chain**

   * Initialize a chat LLM (e.g. `gpt-4o-mini`) via OpenAI.
   * Wrap LLM + retriever in a **ConversationalRetrievalChain** (LangChain),
     with optional conversation memory.

4. **User Interface**

   * Provide a Gradio chat frontend.
   * User question → RAG chain → answer + optional references.

### 4.2 Data Flow

1. User types a question in Gradio.

2. System embeds the question via `text-embedding-3-small`.

3. FAISS returns top-N similar chunks from the KB.

4. System constructs a prompt:

   * System instructions (e.g. “Use only the context below”)
   * Retrieved chunks as **Context**
   * User’s original question

5. LLM generates an answer grounded in the provided context.

6. The answer (and optionally the most relevant sources) is displayed back in the UI.

---


## 5. Software Requirements
###  Python environment

- Python 3.10+ is recommended.
- Create and activate a virtual environment (optional but recommended).

```bash
python -m venv .venv
source .venv/bin/activate        # on Windows: .venv\Scripts\activate
```

###  Dependencies

To install all required packages:

```bash
pip install -r requirements.txt
```

### Configure OpenAI

Set your OpenAI API key as an environment variable:

```bash
export OPENAI_API_KEY="your_key_here"    # Linux / macOS
# PowerShell (Windows):
# $env:OPENAI_API_KEY="your_key_here"
```

### Run the application

After installing dependencies and setting the API key:

```bash
python ingest.py   # build the FAISS index
python app.py      # start the Gradio web app
```

Open the URL shown in the terminal (e.g. `http://127.0.0.1:7860`) to use the InsureLM RAG assistant.

---

## 6. Limitations

1. **Synthetic Data Only (in Prototype)**

   * The dataset is entirely generated by LLMs.
   * No real customers, employees, or contracts are used.
   * This is intentional for privacy, but limits how realistic the insights are.

2. **External Dependency on OpenAI**

   * Embeddings and LLM calls depend on OpenAI’s availability, latency, and pricing.
   * No offline mode in this version.

3. **Scope of Knowledge**

   * The system only knows what is in the markdown files.
   * If content is missing or incomplete, the model may say “I don’t know” or give partial answers.


## 7. Future Improvements

* Replace or complement OpenAI models with **self-hosted LLMs** (e.g., via Ollama or HuggingFace).
* Add **role-based access control** and separate indices per department.
* Improve **retrieval quality**:

  * use more advanced chunking (by paragraphs, headings)
  * use re-ranking models on top of FAISS

